{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minetailingsdams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPyil9F2qzH/skEB0zP51p9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remis/TailingDams/blob/master/minetailingsdams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE8_YNK7eSdi",
        "colab_type": "text"
      },
      "source": [
        "# Mines and tailings dams dicovery and classification\n",
        "\n",
        "This script is part of a research project published on the paper \"Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning\" by Remis Balaniuk, Olga Isupova and Steven Reece. This project was developed at the University of Oxford from September 2019 to February 2020.\n",
        "It was prepared to be used on the Google Colaboratory platform (see https://colab.research.google.com/notebooks/welcome.ipynb ).  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZN7-cp2euZk",
        "colab_type": "text"
      },
      "source": [
        "Use GPU acceleration (Runtime >> Change runtime type) !!\n",
        "\n",
        "Initial settings: import libraries and mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqo4fDgNeEpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install rasterio\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install simplekml\n",
        "import simplekml\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil as GPU\n",
        "import rasterio\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import gc\n",
        "import glob\n",
        "import pickle as pi\n",
        "import re\n",
        "import math\n",
        "from random import shuffle\n",
        "import gdal\n",
        "import os\n",
        "from tempfile import NamedTemporaryFile\n",
        "import h5py\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "rseed = 100\n",
        "np.random.seed(rseed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b3jhzTqe8Z4",
        "colab_type": "text"
      },
      "source": [
        "## Define environment for I/O:\n",
        "\n",
        "This script assumes that the user created on his google drive (mounted as /content/drive/My Drive/) a folder named \"DataForTailingDamDetection/\".\n",
        "\n",
        "Inside this folder three subfoders willl be used: Data/, Experiments/ and Results/. The first one should be used to store images for training, testing and predicting, uploaded by the user. The second will be used to store the trained models and the last one will keep predicting results. \n",
        "\n",
        "The analytical approach is organized in \"experiments\". Each experiment can have its own dataset, model and results. An experiment will have its own subfoder on each of the three main folders: Data/, Experiments/ and Results/. \n",
        "\n",
        "For each new experiment the user must create a new subfolder under the folder  Data/ named after his experiment. Inside this Data/\\<experiment name\\>/ folder the user will create two subfolders: train/ and predict/. The train/ folder must be used to put all training and testing images. The images must be organized in subfolders, with one subfolder for each class. On Data/\\<experiment name\\>/predict/ the user will store the large satellite images for which predicting will be done.\n",
        "\n",
        "The remaining folders will be created automatically by the script.\n",
        "\n",
        "Two experiments were described on the paper  \"Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning\". The first implements mines and tailings dams discovery and the second classifies the ore type of discovery mines and tailings dams. This is the folder's tree for the project experiments:\n",
        "\n",
        "![](https://drive.google.com/file/d/1GdqNvfEzfb5JCG7YYirThg9aUHl8TM26/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i01G_2kle5Vd",
        "colab_type": "code",
        "outputId": "7be5fc3b-0bbf-4ddf-a2df-99edf5f2ddc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " %cd /content/drive/My Drive/\n",
        "\n",
        "# root folder for the project data\n",
        "path_ref = './DataForTailingDamDetection/'\n",
        "sys.path.extend([path_ref])\n",
        "\n",
        "data_path = 'Data/'\n",
        "exp_names = []\n",
        "\n",
        "#-----------------------------------------------\n",
        "# user definitions\n",
        "MINE_IMAGE_WIDTH = 201\n",
        "MINE_IMAGE_HEIGHT = 201\n",
        "ORE_IMAGE_WIDTH = 21\n",
        "ORE_IMAGE_HEIGHT = 21\n",
        "N_BANDS = 12\n",
        "# names of the image folders\n",
        "experiment_name = 'SentinelMinesDams'  \n",
        "oretypeclassifier = 'SentinelOreType' # set to None if ore classifier is not required\n",
        "#-----------------------------------------------\n",
        "\n",
        "experiments_path = 'Experiments/'\n",
        "result_path = 'Results/'\n",
        "if not os.path.exists(path_ref + experiments_path):\n",
        "    os.makedirs(path_ref + experiments_path)\n",
        "if not os.path.exists(path_ref + result_path):\n",
        "    os.makedirs(path_ref + result_path)\n",
        "\n",
        "train_data = 'train/'   \n",
        "predict_data =  'predict/' \n",
        "\n",
        "# user must create the train_data folder and put the training images on subfolders (one per class)\n",
        "path_to_images = path_ref + data_path + experiment_name + '/' + train_data \n",
        "path_to_oretype_samples = path_ref + data_path + oretypeclassifier + '/' + train_data \n",
        "\n",
        "# if user wants to use model to predict on large images he must create the predict folder and put the images \n",
        "path_to_predict_images = path_ref + data_path + experiment_name + '/' + predict_data\n",
        "\n",
        "# will save training results here\n",
        "path_to_output = path_ref + experiments_path + experiment_name + '/'\n",
        "path_to_oretype_output = path_ref + experiments_path + oretypeclassifier + '/'\n",
        "\n",
        "#will save prediction results here\n",
        "path_to_results = path_ref + result_path + experiment_name + '/'\n",
        "if not os.path.exists(path_to_results):\n",
        "    os.makedirs(path_to_results)\n",
        "if not os.path.exists(path_to_output):\n",
        "    os.makedirs(path_to_output)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMlwsw_Yhwrb",
        "colab_type": "text"
      },
      "source": [
        "Before following this script make sure that the images for training were uploaded to the \"path_to_images\" folder. For each class define a subfoder to store its images. To obtain the images on Google Earth Engine use the loadminetailingsdam.ipynb notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owqPjUm_jBpO",
        "colab_type": "text"
      },
      "source": [
        "## Declaring functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrQXbn9SjxoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global n_classes\n",
        "\n",
        "def custom_sparse_categorical_accuracy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes])   # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        "  \n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true,y_pred)\n",
        "\n",
        "def custom_sparse_categorical_crossentropy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes]) # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        " \n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n",
        "\n",
        "def count_images(path_to_samples):\n",
        "  n_images = 0\n",
        "  print(path_to_samples)\n",
        "  minpf = None\n",
        "  cd = 0\n",
        "  for label_num, subdir in enumerate(next(os.walk(path_to_samples))[1]):\n",
        "    x = len(os.listdir(path_to_samples + subdir))\n",
        "    print(\"dir:\",path_to_samples + subdir,x)\n",
        "    n_images += x\n",
        "    cd += 1\n",
        "    if minpf is None or x<minpf:\n",
        "      minpf=x\n",
        "\n",
        "  return n_images,minpf,cd\n",
        "\n",
        "def load_images_as_training_data(path_to_samples,experiment_type):\n",
        "\n",
        "  if experiment_type==0:\n",
        "    image_width = MINE_IMAGE_WIDTH\n",
        "    image_height = MINE_IMAGE_HEIGHT\n",
        "  else:\n",
        "    image_width = ORE_IMAGE_WIDTH\n",
        "    image_height = ORE_IMAGE_HEIGHT\n",
        "\n",
        "  Nbands = N_BANDS\n",
        "\n",
        "  # this script assumes that Sentinel 2 12 spectral bands images are being used\n",
        "  opt_string = '-outsize {} {} -b 1 -b 2 -b 3 -b 4 -b 5 -b 6 -b 7 -b 8 -b 9 -b 10 -b 11 -b 12'.format(image_height, image_width)\n",
        "  qtd_images,min_per_folder,dirs = count_images(path_to_samples)\n",
        "\n",
        "  # loads the same number of images from each folder\n",
        "  print(\"Will load {} images from each folder.\".format(min_per_folder))\n",
        "  n_images = min_per_folder*dirs\n",
        "\n",
        "  image = np.zeros((n_images, image_width, image_height, Nbands), dtype=np.float32)\n",
        "  label = -1 * np.ones((n_images,), dtype=np.int64)\n",
        "  char_labels = []\n",
        "\n",
        "  image_counter = 0\n",
        "  for label_num, subdir in enumerate(next(os.walk(path_to_samples))[1]):\n",
        "    char_labels.append(subdir)\n",
        "\n",
        "    print(\"Loading images for class\",path_to_samples + subdir)\n",
        "\n",
        "    files = np.array(os.listdir(path_to_samples + subdir))\n",
        "    permutation = np.random.permutation(len(files))\n",
        "    files = files[permutation]\n",
        "    count_this_folder = 0\n",
        "    for file in files:\n",
        "        img_dataset = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                     gdal.Open(path_to_samples + subdir + '/' + file, gdal.GA_ReadOnly),\n",
        "                                     options=opt_string)\n",
        "\n",
        "        img_array = np.transpose(np.array(img_dataset.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "\n",
        "        image[image_counter, :, :, :] = img_array[:,:,0:Nbands]\n",
        "        label[image_counter] = label_num\n",
        "        image_counter += 1\n",
        "        count_this_folder +=1\n",
        "        if count_this_folder>=min_per_folder:   # keeping the same number of samples for all classes\n",
        "          break\n",
        "\n",
        "  return n_images,image,label,char_labels,image_width,image_height,Nbands,experiment_type\n",
        "    \n",
        "def save_training_data_to_file(path_to_model,image,label,charlabel,image_width,image_height,nmean,nmax,Nbands,n_classes,experiment_type):\n",
        "  print(\"Saving data:\",charlabel,path_to_model)\n",
        "  np.savez(path_to_model + 'trainingData',\n",
        "             images=image,\n",
        "             labels=label,\n",
        "             charlabels = charlabel)\n",
        "  \n",
        "  np.savez(path_to_model + 'modelMetaData',\n",
        "             metadata =np.array([image_width,image_height,Nbands,n_classes,charlabel,experiment_type])) \n",
        "\n",
        "  save_normal_std(path_to_model,nmean,nmax)\n",
        "  \n",
        "def load_training_data_from_file(path_to_model):\n",
        "  npzfile = np.load(path_to_model + 'trainingData.npz')\n",
        "  image = npzfile['images']\n",
        "  label = npzfile['labels']\n",
        "  charlabel = npzfile['charlabels']\n",
        " \n",
        "  n_images = image.shape[0]\n",
        "  metadata = np.load(path_to_model + 'modelMetaData.npz',allow_pickle=True)\n",
        "  image_width,image_height,n_bands,n_classes,char_labels,experiment_type = metadata['metadata'] \n",
        "\n",
        "  # training data was saved already normalized\n",
        "  nmean,nmax = load_normal_std(path_to_model)\n",
        "\n",
        "  return image,label,charlabel,n_images,image_width,image_height,n_bands,nmean,nmax,experiment_type\n",
        "\n",
        "def normalize_data(path_to_model,image,Nbands,nmean,nmax,save = True,loadfrom = None):\n",
        "  \n",
        "  given = True\n",
        "  if nmean is None:\n",
        "    if loadfrom is not None:\n",
        "      nmean,nmax = load_normal_std(loadfrom)\n",
        "    else:\n",
        "      nmean = np.zeros(Nbands)\n",
        "      nmax = np.zeros(Nbands)\n",
        "      given = False\n",
        "\n",
        "  # Normalize pixel values to be between 0 and 1\n",
        "  for i in range(Nbands):\n",
        "    if not given:\n",
        "      nmean[i] = np.mean(image[:,:,:,i])\n",
        "      nmax[i] = np.std(image[:,:,:,i])\n",
        "    image[:,:,:,i]=image[:,:,:,i]-nmean[i]\n",
        "    image[:,:,:,i]=image[:,:,:,i]/nmax[i]\n",
        "\n",
        "  if save:\n",
        "    save_normal_std(path_to_model,nmean,nmax)\n",
        "\n",
        "  return image,nmean,nmax\n",
        "\n",
        "def save_normal_std(path_to_model,nmean,nmax):\n",
        "  np.savez(path_to_model + 'trainingDataAvgStd',\n",
        "             median=nmean,\n",
        "             std=nmax)\n",
        "  \n",
        "def load_normal_std(path_to_model):\n",
        "  npzfile = np.load(path_to_model + 'trainingDataAvgStd.npz',)\n",
        "  nmean = npzfile['median']\n",
        "  nmax = npzfile['std']\n",
        "\n",
        "  return nmean,nmax\n",
        "\n",
        "def convert_label_to_one_hot(labels, n_classes):\n",
        "    labels_arr = np.atleast_1d(labels)  # in case labels is a scalar\n",
        "    one_hot_labels = np.zeros((labels_arr.shape[0], n_classes), dtype = np.int)\n",
        "    one_hot_labels[np.arange(labels_arr.shape[0]), labels_arr] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "def split_training_test_sets(image,label,split_ratio):\n",
        "  # split_ratio defines the training share of the data. Must be between 0 and 1.\n",
        "\n",
        "  n_images,image_width,image_height,Nbands = image.shape\n",
        "\n",
        "  permutation = np.random.permutation(n_images)\n",
        "  image = image[permutation]\n",
        "  label = label[permutation]\n",
        "\n",
        "  Ntrain = int(n_images*split_ratio)\n",
        "  max_Ntrain = n_images\n",
        "\n",
        "  ls = list(label.shape)\n",
        "\n",
        "  ls[0]=Ntrain\n",
        "  train_images = np.zeros((Ntrain,image_width,image_height,Nbands))\n",
        "  train_labels = np.zeros(tuple(ls)) # (Ntrain,))\n",
        "\n",
        "  ls[0]=max_Ntrain - Ntrain\n",
        "  test_images = np.zeros((max_Ntrain - Ntrain,image_width,image_height,Nbands))\n",
        "  test_labels = np.zeros(tuple(ls)) #(max_Ntrain - Ntrain,))\n",
        "\n",
        "  train_images[:Ntrain] = image[:Ntrain] \n",
        "  train_labels[:Ntrain] = label[:Ntrain] \n",
        "\n",
        "  test_images[:] = image[Ntrain:]\n",
        "  test_labels[:] = label[Ntrain:]\n",
        "\n",
        "  return train_images,train_labels,test_images,test_labels\n",
        "\n",
        "def define_FCN_discovery(image_width,image_height,nc,Nbands):\n",
        "\n",
        "  global n_classes\n",
        "  n_classes = nc\n",
        "  kx = math.ceil((image_width-32)/27)  \n",
        "  ky = math.ceil((image_height-32)/27)  \n",
        "  # define the Fully Convolution Neural Network\n",
        "  input=layers.Input(shape=(None, None, Nbands))\n",
        "  x0=layers.Conv2D(32, (3, 3), (1,1), activation='relu', padding='SAME')(input)\n",
        "  x1=layers.MaxPooling2D((3, 3))(x0)\n",
        "  x2=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x1)\n",
        "  x3=layers.MaxPooling2D((3, 3))(x2)\n",
        "  x4=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x3)\n",
        "  x5=layers.MaxPooling2D((3,3))(x4)\n",
        "  x6=layers.Conv2D(64, (kx,ky), (1,1), activation = 'relu')(x5)  # the convolution kernel size must be adapted for image size \n",
        "  x7=layers.Dropout(0.5)(x6)\n",
        "  x8=layers.Conv2D(n_classes, (1,1), (1,1), padding='VALID', activation='softmax')(x7)\n",
        "  model=models.Model(input,x8)\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "              loss=custom_sparse_categorical_crossentropy,\n",
        "              metrics=[custom_sparse_categorical_accuracy])\n",
        "             \n",
        "  return model\n",
        "\n",
        "def tf_cross_entropy_with_logits(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
        "\n",
        "def define_FCN_oretype(image_width,image_height,nc,Nbands):\n",
        "\n",
        "  global n_classes\n",
        "  n_classes = nc\n",
        "  \n",
        "  input_shape=(image_width, image_height, Nbands)\n",
        "  print(input_shape)\n",
        "  cnn_model = tf.keras.models.Sequential()\n",
        "  cnn_model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(32, (2, 2), strides=1, padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(64, (2, 2), strides=1, padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Flatten())\n",
        "  cnn_model.add(tf.keras.layers.Dense(1024))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "  cnn_model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
        "  cnn_model.summary()\n",
        "\n",
        "  adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
        "\n",
        "  cnn_model.compile(optimizer=adam, loss=tf_cross_entropy_with_logits, metrics=['accuracy'])\n",
        "\n",
        "  return cnn_model\n",
        "             \n",
        "\n",
        "def fit_model(path_to_model,model, train_images, train_labels, epochs = 20, batchsize = 16, plot_accuracy = True, save_model = True):\n",
        "\n",
        "  # Create a callback that saves the model's weights\n",
        "  if save_model:\n",
        "\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath= path_to_model + 'trained_model/weights.ckpt',\n",
        "                                                    save_weights_only=True,\n",
        "                                                    verbose=1)\n",
        "\n",
        "    fit = model.fit(train_images, train_labels, epochs=epochs, batch_size=batchsize,callbacks=[cp_callback])\n",
        "  else:\n",
        "    fit = model.fit(train_images, train_labels, epochs=epochs, batch_size=batchsize)\n",
        "\n",
        "  accuracy = fit.history[list(fit.history.keys())[-1]]\n",
        "\n",
        "  if plot_accuracy:\n",
        "    plt.plot(range(epochs), accuracy, 'r--', label='train')\n",
        "    plt.title('Training accuracy')\n",
        "    plt.show()\n",
        "\n",
        "  return model,accuracy\n",
        "\n",
        "def custom_fit(path_to_model,model_type, image, label,image_width,image_height,n_classes,Nbands,batchsize = 16, plot_accuracy = True, save_model = True):\n",
        "\n",
        "  print(\"How many folds for cross validation? \")\n",
        "  try:\n",
        "    n_cv_runs=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  print(\"How many epochs at each fold ? \")\n",
        "  try:\n",
        "    n_epoch=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  bestaccuracy = -1\n",
        "  accuracyTrain = np.zeros((n_cv_runs,n_epoch))\n",
        "  accuracyTest = np.zeros((n_cv_runs,n_epoch))\n",
        "  f1Score= np.zeros((n_cv_runs,n_epoch))\n",
        "  confusionMatrix = np.zeros((n_cv_runs,n_epoch,n_classes,n_classes))\n",
        "  models = []\n",
        "  # define model and compile\n",
        "  if model_type==0:\n",
        "    model = define_FCN_discovery(image_width,image_height,n_classes,Nbands)\n",
        "  else:\n",
        "    model = define_FCN_oretype(image_width,image_height,n_classes,Nbands)\n",
        "  # save initial state\n",
        "  weights = model.get_weights() \n",
        "\n",
        "  # shuffle image set\n",
        "  permutation = np.random.permutation(len(image))\n",
        "  image = image[permutation]\n",
        "  label = label[permutation]\n",
        "\n",
        "  kf = KFold(n_splits=n_cv_runs)\n",
        " \n",
        "  cv_run = 0\n",
        "  for train_idx, val_idx in kf.split(image,label):\n",
        "    train_images = image[train_idx]\n",
        "    train_labels = label[train_idx]\n",
        "    test_images = image[val_idx]\n",
        "    test_labels = label[val_idx]\n",
        "\n",
        "    print('\\tfold %d...' % cv_run)\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "      print('\\tepoch %d...' % epoch)\n",
        "\n",
        "      fit = model.fit(train_images, train_labels, epochs=1, batch_size=batchsize)\n",
        "      \n",
        "      accuracyTrain[cv_run,epoch] = fit.history[list(fit.history.keys())[-1]][-1]\n",
        "\n",
        "      # testing\n",
        "      prediction = model.predict(test_images)\n",
        "      predics = np.zeros(len(prediction))\n",
        "      if model_type==0:\n",
        "        predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "        tl = test_labels \n",
        "      else:\n",
        "        predics = prediction[:,:].argmax(axis=1) \n",
        "        tl = test_labels.argmax(axis=1)\n",
        "\n",
        "      # metrics\n",
        "      accuracyTest[cv_run,epoch] = accuracy_score(tl, predics)\n",
        "      if n_classes == 2:\n",
        "        f1Score[cv_run,epoch]  = f1_score(tl, predics)\n",
        "      confusionMatrix[cv_run,epoch] = confusion_matrix(tl, predics)\n",
        "      \n",
        "    print(\"Final metrics on fold\",cv_run,\": accuracy on training:\",accuracyTrain[cv_run,-1],\"accuracy on testing:\",accuracyTest[cv_run,-1])\n",
        "    if n_classes == 2:\n",
        "      print(\"F1 score:\",f1Score[cv_run,-1])\n",
        "    print(\"Confusion matrix:\",confusionMatrix[cv_run,-1])\n",
        "\n",
        "    plt.plot(range(n_epoch), accuracyTrain[cv_run], 'r--', label='train')\n",
        "    plt.plot(range(n_epoch), accuracyTest[cv_run], 'b-', label='test')\n",
        "    plt.title('Accuracy {} run'.format(cv_run))\n",
        "    plt.savefig(path_to_model + 'accuracy_results_{}.pdf'.format(cv_run))\n",
        "    plt.show()\n",
        "\n",
        "    model.save(path_to_model + 'trained_model/weights_{}_run.ckpt'.format(cv_run))\n",
        " \n",
        "    np.savez(path_to_model + 'results_{}_run'.format(cv_run),\n",
        "              train_accuracy=accuracyTrain[cv_run],\n",
        "              f1Score=f1Score[cv_run],\n",
        "              test_accuracy=accuracyTest[cv_run],\n",
        "              confusionMatrix=confusionMatrix[cv_run])\n",
        "    \n",
        "    # return model to initial state\n",
        "    model.set_weights(weights)\n",
        "    del train_images,train_labels,test_images,test_labels\n",
        "    gc.collect()\n",
        "    cv_run+=1\n",
        "\n",
        "  np.savez(path_to_model + 'results_final',\n",
        "              train_accuracy=accuracyTrain,\n",
        "              f1Score=f1Score,\n",
        "              test_accuracy=accuracyTest,\n",
        "              confusionMatrix=confusionMatrix)\n",
        "\n",
        "  print(\"Summary:\")\n",
        "  for cv_run in range(n_cv_runs):\n",
        "    plt.plot(range(n_epoch), accuracyTrain[cv_run], 'r--', label='train{}_run'.format(cv_run))\n",
        "    plt.plot(range(n_epoch), accuracyTest[cv_run], 'b-', label='test{}_run'.format(cv_run))\n",
        "    print(\"Fold\",cv_run,\": accuracy on training:\",accuracyTrain[cv_run,-1],\"accuracy on testing:\",accuracyTest[cv_run,-1],\"F1 score:\",f1Score[cv_run,-1])\n",
        "\n",
        "  plt.title('Accuracy final')\n",
        "  plt.savefig(path_to_model + 'accuracy_results_final.pdf')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Which model do you want to use?\")\n",
        "  try:\n",
        "    id=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "  # load the chosen model from disk\n",
        "  model.load_weights(path_to_model + 'trained_model/weights_{}_run.ckpt/variables/variables'.format(id))\n",
        "\n",
        "  return model\n",
        "\n",
        "def load_weights(path_to_model,model):\n",
        "    \n",
        "  if os.path.exists(path_to_model + 'trained_model/checkpoint'): \n",
        "    print(\"Single trained model found. Do you want to load it? (1:yes/0:no)\" )\n",
        "    try:\n",
        "      resp=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "    if resp==1:\n",
        "      model.load_weights(path_to_model + 'trained_model/weights.ckpt')\n",
        "      return model\n",
        "\n",
        "  if os.path.exists(path_to_model + 'results_final.npz'): # check if there is k-fold saved models available\n",
        "    results = np.load(path_to_model + 'results_final.npz')\n",
        "    print(\"K-fold cross validation trained models found:\")\n",
        "    for i in range(0,len(results['train_accuracy'])):\n",
        "      print('Model {}: train accuracy = {} test accuracy = {} f1 score = {}'.format(i,results['train_accuracy'][i,-1],results['test_accuracy'][i,-1],results['f1Score'][i,-1]))\n",
        "    print(\"Which one do you want to load?\" )\n",
        "    try:\n",
        "      resp=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "    if resp < 0 or resp>len(results['train_accuracy']):\n",
        "      raise Exception(\"Invalid model choice\")\n",
        "\n",
        "    model.load_weights(path_to_model + 'trained_model/weights_{}_run.ckpt/variables/variables'.format(resp))\n",
        "\n",
        "    return model\n",
        "\n",
        "  raise Exception(\"No model loaded\")\n",
        "\n",
        "def load_model(path_to_model):\n",
        "  \n",
        "  metadata = np.load(path_to_model + 'modelMetaData.npz',allow_pickle=True)\n",
        "  image_width,image_height,n_bands,n_classes,char_labels,experiment_type = metadata['metadata'] \n",
        "\n",
        "  if experiment_type==0:  # dams discovery  \n",
        "    model = define_FCN_discovery(image_width,image_height,n_classes,n_bands)\n",
        "  else:\n",
        "    model = define_FCN_oretype(image_width,image_height,n_classes,n_bands)\n",
        "\n",
        "  model= load_weights(path_to_model,model)\n",
        "  nmean,nmax = load_normal_std(path_to_model)\n",
        "  \n",
        "  return model,image_width,image_height,n_bands,n_classes,nmean,nmax,char_labels,experiment_type\n",
        "\n",
        "def train_test(path_to_model,model_type,image,label,image_width,image_height,n_classes,Nbands,clearMemory = True):\n",
        "\n",
        "  if model_type==1:  # not a FCN\n",
        "    label = convert_label_to_one_hot(label,n_classes)\n",
        "\n",
        "  print(\"Training and testing (0.8 - 0.2 split):1, training for predicting (no split, no testing): 2 or customized learning : 3 ? ((1/2/3))\")\n",
        "  try:\n",
        "    r=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  if r==3:\n",
        "    model = custom_fit(path_to_model,model_type, image,label,image_width,image_height,n_classes,Nbands)\n",
        "  else:\n",
        "    if model_type==0:\n",
        "      model = define_FCN_discovery(image_width,image_height,n_classes,Nbands)\n",
        "    else:\n",
        "      model = define_FCN_oretype(image_width,image_height,n_classes,Nbands)\n",
        "\n",
        "    if r == 1:\n",
        "      train_images,train_labels,test_images,test_labels = split_training_test_sets(image,label,0.8)\n",
        "      model,accuracy = fit_model(path_to_model,model, train_images, train_labels)\n",
        "      # testing\n",
        "      print(\"Testing:\")\n",
        "      prediction = model.predict(test_images)\n",
        "      \n",
        "      predics = np.zeros(len(prediction))\n",
        "      if model_type==0:\n",
        "        predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "      else:\n",
        "        predics = prediction[:,:].argmax(axis=1)\n",
        "        test_labels = test_labels[:,:].argmax(axis=1)\n",
        "\n",
        "      print(confusion_matrix(test_labels, predics))\n",
        "      \n",
        "      del test_images\n",
        "      del test_labels\n",
        "      del train_images\n",
        "      del train_labels\n",
        "    else: \n",
        "      model,accuracy = fit_model(path_to_model,model, image,label)\n",
        "      print(\"Final accuracy:\",accuracy[-1])\n",
        "\n",
        "  if clearMemory:\n",
        "    del image\n",
        "    del label\n",
        "    \n",
        "  gc.collect()\n",
        "\n",
        "  return model\n",
        "\n",
        "def predict(main_model,ore_classifier,n_images,main_labels,ore_labels,image_width,image_height,nmean,nmax,Nbands,slicing = 1):\n",
        "\n",
        "  countImages = 0\n",
        "  \n",
        "  print(\"Choose target class: \")\n",
        "  for i in range(0, len(main_labels)):\n",
        "    print(i,\":\",main_labels[i])\n",
        "  try:\n",
        "    save=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  print(\"Choose the no ore class: \")\n",
        "  for i in range(0, len(ore_labels)):\n",
        "    print(i,\":\",ore_labels[i])\n",
        "  try:\n",
        "    noore=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  kmlFull=simplekml.Kml()\n",
        "  resultsFull = [] \n",
        "\n",
        "  # process each image on the predict folder\n",
        "  for file in os.listdir(path_to_predict_images):\n",
        "    print(file)\n",
        "    starthere = len(resultsFull)\n",
        "\n",
        "    testimage = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                  gdal.Open(path_to_predict_images + file, gdal.GA_ReadOnly),\n",
        "                                  options = '-b 1 -b 2 -b 3 -b 4 -b 5 -b 6 -b 7 -b 8 -b 9 -b 10 -b 11 -b 12')\n",
        "\n",
        "    testimage = np.transpose(np.array(testimage.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "    testimage = testimage[:,:,0:Nbands]\n",
        "    for i in range(Nbands):\n",
        "      testimage[:,:,i]=testimage[:,:,i]-nmean[i]\n",
        "      testimage[:,:,i]=testimage[:,:,i]/nmax[i]\n",
        "\n",
        "    # slice the image for prediction if it is to too large for colab. \n",
        "    # Divides the image and regular 2D patches\n",
        "    # Adapt the \"slicing\" for your size of image and colab memory availability:  \n",
        "    \n",
        "    sh = np.array(testimage.shape)\n",
        "    sh[0] = sh[0]//slicing\n",
        "    sh[1] = sh[1]//slicing\n",
        "    prediction = None\n",
        "    for i in range(0,slicing):  \n",
        "      for j in range(0,slicing):\n",
        "        patch = np.expand_dims(testimage[sh[0]*i:sh[0]*(i+1),sh[1]*j:sh[1]*(j+1),:],axis=0) \n",
        "        # compute prediction of the FCN\n",
        "        if prediction is None:\n",
        "          prediction = main_model.predict(patch)\n",
        "        else:\n",
        "          prediction = np.append(prediction,main_model.predict(patch), axis=0)\n",
        "      \n",
        "    # locate the predictions on the patches\n",
        "\n",
        "    shp = prediction.shape\n",
        "    results = []\n",
        "    lpatches = []\n",
        "    kml=simplekml.Kml()\n",
        "    count_positives = 0\n",
        "\n",
        "    with rasterio.open(path_to_predict_images+file) as map_layer:\n",
        "      for i in range(0,slicing):  \n",
        "        for j in range(0,slicing): \n",
        " \n",
        "          pos = i*slicing+j\n",
        "          \n",
        "          pred_classes = tf.greater(prediction[pos,:,:,save],0.6) # considering a 60% probability threshold to accept a positive\n",
        "          shpc = tf.shape(pred_classes)\n",
        "\n",
        "          inds = tf.where(pred_classes==True)\n",
        "\n",
        "          # Upsampling to original image dimensions.\n",
        "          \n",
        "          xloc = i*sh[0]+np.array(tf.gather(np.linspace(image_width/2.0,sh[0]-image_width/2.0,num=shpc[0]),inds[:,0]))\n",
        "          yloc = j*sh[1]+np.array(tf.gather(np.linspace(image_height/2.0,sh[1]-image_height/2.0,num=shpc[1]),inds[:,1]))\n",
        "          predloc = prediction[pos,inds[:,0],inds[:,1],save]\n",
        "\n",
        "          for k in range(0,len(xloc)):\n",
        "            pixels2coords = map_layer.xy(xloc[k] , yloc[k]) \n",
        "\n",
        "            r = np.ones(4)*-1      \n",
        "            \n",
        "            r[0] = pixels2coords[0]\n",
        "            r[1] = pixels2coords[1]\n",
        "            r[2] = predloc[k]\n",
        "\n",
        "            # prepare ore type prediction\n",
        "            if ore_classifier is not None: \n",
        "              #lx = int(round(xloc[k]-(ORE_IMAGE_WIDTH)))\n",
        "              lx = int(round(xloc[k]-(ORE_IMAGE_WIDTH//2)))\n",
        "              if lx<0:\n",
        "                lx=0\n",
        "              #ly = int(round(yloc[k]-(ORE_IMAGE_HEIGHT)))\n",
        "              ly = int(round(yloc[k]-(ORE_IMAGE_HEIGHT//2)))\n",
        "              if ly<0:\n",
        "                ly=0\n",
        "              hx = lx+ORE_IMAGE_WIDTH\n",
        "              if hx>testimage.shape[0]:\n",
        "                hx=testimage.shape[0]\n",
        "              \n",
        "              hy = ly+ORE_IMAGE_HEIGHT\n",
        "              if hy>testimage.shape[1]:\n",
        "                hy=testimage.shape[1]\n",
        "              \n",
        "              lpatch = np.expand_dims(testimage[lx:hx,ly:hy,:],axis=0)  \n",
        "              ore_prediction = ore_classifier.predict(lpatch)\n",
        "              \n",
        "              # ore type winner\n",
        "              r[-1] = ore_prediction[0,:].argmax()\n",
        "              tag = ore_labels[r[-1].astype(int)]\n",
        "            else:\n",
        "              tag = main_labels[save]\n",
        "              \n",
        "            if r[-1] != noore: # if no ore was detected discard point\n",
        "              kml.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "              kmlFull.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "\n",
        "              results.append(r)\n",
        "              resultsFull.append(r)\n",
        "\n",
        "    # save prediction files to the results folder\n",
        "    np.savetxt(path_to_results + 'prediction_{}.csv'.format(file), results, delimiter=\",\")\n",
        "    kml.save(path_to_results + 'predictions_{}.kml'.format(file))  \n",
        "\n",
        "    countImages+=1\n",
        "\n",
        "    del testimage\n",
        "    del patch\n",
        "    del prediction\n",
        "    del results\n",
        "    del kml\n",
        "    gc.collect()\n",
        "\n",
        "    if n_images != -1 and countImages>=n_images:\n",
        "      break\n",
        "\n",
        "  np.savetxt(path_to_results + 'prediction_{}.csv'.format(experiment_name), np.array(resultsFull), delimiter=\",\")\n",
        "  kmlFull.save(path_to_results + 'predictions_{}.kml'.format(experiment_name))  \n",
        "\n",
        "def model_workflow(path_to_model, path_to_images, model_type,main_model = None,nmean=None,nmax=None):\n",
        "\n",
        "  load = False\n",
        "  if os.path.exists(path_to_model + 'trained_model/checkpoint') or os.path.exists(path_to_model + 'results_final.npz'): # check if there is a saved model available\n",
        "    print(\"Saved model found. Do you want to load it? ((1:yes/0:no))\")\n",
        "    try:\n",
        "      r=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "    if r == 1:\n",
        "      model,image_width,image_height,Nbands,n_classes,nmean,nmax,char_labels,experiment_type = load_model(path_to_model)\n",
        "      load = True\n",
        "\n",
        "  if not load:  # training new model\n",
        "    # load training data from images or from array\n",
        "    loaded = False\n",
        "    if os.path.exists(path_to_model + 'trainingData.npz'):\n",
        "      print(\"Saved training data found. Do you want to load it? ((1:yes/0:no))\")\n",
        "      try:\n",
        "        r=int(input('Input:'))\n",
        "      except ValueError:\n",
        "        print(\"Not a number\")\n",
        "      if r == 1:\n",
        "        # loading data from array already normalized\n",
        "        image,label,char_labels,n_images,image_width,image_height,Nbands,nmean,nmax,experiment_type = load_training_data_from_file(path_to_model)\n",
        "        loaded = True\n",
        "\n",
        "    if not loaded: # load data from images and normalize\n",
        "\n",
        "      n_images,image,label,char_labels,image_width,image_height,Nbands,experiment_type = load_images_as_training_data(path_to_images,model_type)\n",
        "      image,nmean,nmax =  normalize_data(path_to_model,image,Nbands,nmean,nmax)\n",
        "\n",
        "      print(\"Training data loaded from the image sets:\",n_images,char_labels)\n",
        "      print(\"Do you want to save images as arrays for faster future loading? ((1:yes/0:no))\")\n",
        "      try:\n",
        "        r=int(input('Input:'))\n",
        "      except ValueError:\n",
        "        print(\"Not a number\")\n",
        "      if r == 1:\n",
        "        save_training_data_to_file(path_to_model,image,label,char_labels,image_width,image_height,nmean,nmax,Nbands,len(char_labels),model_type) # saving normalized data\n",
        "\n",
        "    n_classes = len(char_labels)\n",
        "    print(\"Training data loaded:\",image_width,image_height,Nbands,n_classes,char_labels,np.isnan(np.sum(image)))\n",
        "    model = train_test(path_to_model, model_type,image,label,image_width,image_height,n_classes,Nbands)\n",
        "\n",
        "  return model,char_labels,image_width,image_height,nmean,nmax\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3rFfcgdT9v-",
        "colab_type": "text"
      },
      "source": [
        "## Main workflow:\n",
        "\n",
        "The following script was used to implement the method described on the paper \"Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning\". It trains a first CNN to identify surface mines on satellite images and a second CNN to classify the potential environment impact of the discovered mining spots. If trained models are detected on the \"Experiments\" folder the user can choose to load them instead of retrain new models. Trained or loaded models can then be used to discover and classify mines on large areas represented by satellite images contained  \"predict\" folder. Results are recorded on files made available on the \"Results\" folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OddwSGSEkqL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load or train main experiment (mines an dams discovery)\n",
        "print(\"Discovery model:\")\n",
        "main_model,main_labels,image_width,image_height,nmean,nmax = model_workflow(path_to_output, path_to_images, 0)\n",
        "print(main_labels)\n",
        "# ore type classifier\n",
        "if oretypeclassifier is not None:  #check if ore classifier is required\n",
        "  print(\"Ore classifier :\")\n",
        "  ore_classifier,ore_labels,_,_,_,_ = model_workflow(path_to_oretype_output, path_to_oretype_samples, 1,main_model,nmean,nmax)\n",
        "  print(ore_labels)\n",
        "else:\n",
        "  ore_classifier = None\n",
        "\n",
        "# use model to predict\n",
        "if os.path.exists(path_to_predict_images):\n",
        "  x = len(os.listdir(path_to_predict_images))\n",
        "  if x== 0:\n",
        "    print(\"No images found on the prediction folder\")\n",
        "  else:  \n",
        "    print(x,\"images found on the prediction folder. How many do you want to use?(-1 for all of them)\")\n",
        "    try:\n",
        "      n=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "\n",
        "    print(\"Choose a slicing factor (use 1 for images up to 1GB, will break the TIFF in n^2 pieces)\")\n",
        "    try:\n",
        "      slicing=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "\n",
        "    predict(main_model,ore_classifier,n,main_labels,ore_labels,image_width,image_height,nmean,nmax,N_BANDS,slicing)\n",
        "\n",
        "else:\n",
        "  print(\"Folder containing images for prediction not found \",path_to_predict_images)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl087ecWf2zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}